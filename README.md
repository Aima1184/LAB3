# Лабораторная работа №3

**Тема**: генерация текста в стиле поэта  
**Цель**: познакомиться с задачей генерации текста и применить предобученную модель T5 (Text-To-Text Transfer Transformer) для генерации произведений в стиле выбранного поэта с использованием метода fine-tuning.  
**Студенты**: Чупахина В.В., Фомичева П.Ю., Салимова А.Ф.

## Содержание

- [Теоретическая часть](#теоретическая-часть)
- [Описание разработанной системы](#описание-разработанной-системы-алгоритмы-принципы-работы-архитектура)
- [Результаты](#результаты)
- [Выводы](#выводы)
- [Список использованных источников](#список-использованных-источников)

## Теоретическая часть
### Задача генерации текста

Генерация текста — одна из ключевых задач обработки естественного языка (NLP), направленная на создание логически последовательного, грамматически правильного и семантически осмысленного текста. Она находит применение в различных областях: автоматический перевод, чат-боты, генерация резюме текстов, креативное письмо, программирование и др.
В задаче генерации текста модель получает на вход некоторый контекст (например, начало предложения, набор ключевых слов или вопрос) и должна выдать продолжение текста, которое максимально соответствует семантике и синтаксису входных данных.
Современные методы генерации текста используют архитектуры на основе трансформеров, такие как GPT, BART, T5 и другие. Эти модели обладают способностью захватывать длинные зависимости в тексте и учитывать широкий контекст, что существенно повышает качество генерируемого контента.

### Архитектура T5

T5 (Text-To-Text Transfer Transformer) — это универсальная модель трансформера, предложенная исследователями из Google Research. В отличие от специализированных моделей, T5 предлагает единый подход к решению всех NLP-задач через преобразование любых входных данных в текст и генерацию текстового ответа. Такой подход позволяет унифицировать обучение и использование модели в широком спектре задач.

#### Основные особенности:
- Унифицированный формат задач: все задачи (перевод, суммаризация, классификация, генерация и др.) представлены как задачи преобразования текста в текст.
- Архитектура Encoder-Decoder: энкодер обрабатывает входной текст, а декодер поэтапно генерирует выходной текст.
Снимок экрана 2025-04-14 в 17.42.12
- Механизм предобучения: используется задача "Span Corruption", где случайные фрагменты текста заменяются специальным маркером, а модель обучается восстанавливать их.
- Масштабируемость: модель доступна в различных размерах, от компактных до очень больших (T5-Base, T5-Large, T5-3B и др.).
- Гибкость и адаптивность: легко дообучается на новых задачах с использованием небольшого количества данных (few-shot learning, fine-tuning).
### Подходы к генерации текста
Существует несколько стратегий генерации текста:
- Greedy Search — выбирается токен с наибольшей вероятностью на каждом шаге. Быстро, но может быть однообразно.

- Beam Search — учитывает несколько наиболее вероятных путей генерации. Повышает связность, но снижает разнообразие.
- Sampling (Top-k, Top-p/Nucleus sampling) — случайный выбор токена среди наиболее вероятных. Даёт более разнообразные и креативные тексты.
- Temperature scaling — регулирует уровень "смелости" модели: высокая температура увеличивает разнообразие, низкая — делает вывод более уверенным.


### Метрики оценки качества генерации текста:
Оценка качества сгенерированных текстов является нетривиальной задачей, поскольку она должна учитывать как грамматическую правильность, так и смысловую релевантность. Существует ряд метрик, как автоматических, так и основанных на человеческой оценке:

- **chrF(++)** — символьная метрика, основанная на совпадении n-грамм, подходит для морфологически богатых языков.

- **Perplexity** — измеряет, насколько хорошо языковая модель предсказывает следующий токен. Чем ниже perplexity, тем лучше модель.
- **BLEU / ROUGE** — классические метрики на основе совпадения слов и фраз, применимы к задачам перевода и суммаризации.
- **Distinct-n**  — измеряет разнообразие генерируемых текстов, основываясь на количестве уникальных n-грамм.
- **Novelty** — показывает, насколько новый текст отличается от обучающего корпуса, важен для генерации креативных текстов.
- **LLM-as-judge** — сравнительно новый подход, при котором большая языковая модель (например, GPT-4) используется как оценщик качества других моделей на основе логических, семантических и стилистических критериев.

---
## Описание разработанной системы (алгоритмы, принципы работы, архитектура)
### Подход

Для генерации текстов в стиле выбранного поэта была использована модель T5. Система обучалась на собранном датасете произведений, после чего использовалась для генерации новых текстов.

### Алгоритм работы

1. **Сбор и предобработка данных**:
   - Сбор произведений поэта.
   - Очистка, нормализация, разбиение на обучающие примеры.

2. **Fine-Tuning модели T5**:
   - Загрузка предобученной модели `t5-small`.
   - Дообучение на собственном датасете.
   - Использование tokenization через `T5Tokenizer`.

3. **Генерация текста**:
   - Задание начального текста или команды (`prompt`).
   - Использование метода `generate()` для создания текста.
   - Применение настроек beam search, temperature и т.д.

4. **Оценка качества**:
   - Применение метрик: chrF, perplexity, distinct-n, novelty, LLM-as-judge.
   - Сравнение с реальными произведениями поэта.

---
## Результаты

...

## Выводы

...

## Список использованных источников

1. Raffel, C. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer / C. Raffel, N. Shazeer, A. Roberts [и др.] // arXiv preprint arXiv:1910.10683. – 2020. – URL: https://arxiv.org/abs/1910.10683 (дата обращения: 14.04.2025).
2. Hugging Face Transformers : документация. – 2024 – URL: https://huggingface.co/docs/transformers (дата обращения: 14.04.2025).
3. Google Research. Text-to-Text Transfer Transformer (T5) : репозиторий. – URL: https://github.com/google-research/text-to-text-transfer-transformer (дата обращения: 14.04.2025).
4. Васильев, А. М. Искусственный интеллект и обработка естественного языка : учебное пособие / А. М. Васильев. – Москва : КНОРУС, 2022. – 304 с. – ISBN 978-5-406-09581-2.
5. Бондарев, А. В. Глубокое обучение и трансформеры : от BERT до GPT / А. В. Бондарев. – Санкт-Петербург : Питер, 2023. – 288 с. – ISBN 978-5-4461-1873-1.
